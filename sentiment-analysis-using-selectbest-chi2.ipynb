{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5862f533",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.015766,
     "end_time": "2022-01-24T21:16:07.384942",
     "exception": false,
     "start_time": "2022-01-24T21:16:07.369176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importation des bibliothèques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d359c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:07.423223Z",
     "iopub.status.busy": "2022-01-24T21:16:07.418175Z",
     "iopub.status.idle": "2022-01-24T21:16:14.358475Z",
     "shell.execute_reply": "2022-01-24T21:16:14.357547Z",
     "shell.execute_reply.started": "2022-01-24T21:07:35.355266Z"
    },
    "papermill": {
     "duration": 6.95764,
     "end_time": "2022-01-24T21:16:14.358646",
     "exception": false,
     "start_time": "2022-01-24T21:16:07.401006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3e6be",
   "metadata": {
    "papermill": {
     "duration": 0.014777,
     "end_time": "2022-01-24T21:16:14.387766",
     "exception": false,
     "start_time": "2022-01-24T21:16:14.372989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importation des données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cab4daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:14.422389Z",
     "iopub.status.busy": "2022-01-24T21:16:14.421684Z",
     "iopub.status.idle": "2022-01-24T21:16:14.480225Z",
     "shell.execute_reply": "2022-01-24T21:16:14.479660Z",
     "shell.execute_reply.started": "2022-01-24T20:55:56.025934Z"
    },
    "papermill": {
     "duration": 0.078306,
     "end_time": "2022-01-24T21:16:14.480365",
     "exception": false,
     "start_time": "2022-01-24T21:16:14.402059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/test_stage1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67611a",
   "metadata": {
    "papermill": {
     "duration": 0.013448,
     "end_time": "2022-01-24T21:16:14.507842",
     "exception": false,
     "start_time": "2022-01-24T21:16:14.494394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add6a2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:14.538492Z",
     "iopub.status.busy": "2022-01-24T21:16:14.537850Z",
     "iopub.status.idle": "2022-01-24T21:16:15.342223Z",
     "shell.execute_reply": "2022-01-24T21:16:15.341578Z",
     "shell.execute_reply.started": "2022-01-24T21:00:34.642754Z"
    },
    "papermill": {
     "duration": 0.820792,
     "end_time": "2022-01-24T21:16:15.342361",
     "exception": false,
     "start_time": "2022-01-24T21:16:14.521569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('arabic')\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "translator = str.maketrans('', '', punctuations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8b93ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:15.375088Z",
     "iopub.status.busy": "2022-01-24T21:16:15.374358Z",
     "iopub.status.idle": "2022-01-24T21:16:15.390268Z",
     "shell.execute_reply": "2022-01-24T21:16:15.390783Z",
     "shell.execute_reply.started": "2022-01-24T21:01:12.169834Z"
    },
    "papermill": {
     "duration": 0.033738,
     "end_time": "2022-01-24T21:16:15.390970",
     "exception": false,
     "start_time": "2022-01-24T21:16:15.357232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence_0 = [w for w in word_tokens if not w in stop_words] \n",
    "    text = ' '.join([i for i in filtered_sentence_0])\n",
    "    return text\n",
    "\n",
    "def NormalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return text\n",
    "\n",
    "def arabic_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    st = ISRIStemmer()\n",
    "    stemmed_words = []\n",
    "    word_tokens = word_tokenize(text) \n",
    "    for w in word_tokens:\n",
    "        stemmed_words.append(st.stem(w))\n",
    "    stemmed_words = \" \".join(stemmed_words)\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_english_characters(text):\n",
    "        #out = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "        out= re.sub(r'[a-zA-Z]+', '', text)\n",
    "        #out = re.sub(r\"\\n\", '', out)\n",
    "        #out = re.sub(r\"\\s+\", ' ', out)\n",
    "        #out = re.sub(r'[^\\u0600-\\u06FF]', ' ', out)\n",
    "        return out.strip() \n",
    "\n",
    "def removeLetters(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if len(w)>1] \n",
    "    text = ' '.join([i for i in filtered_sentence])\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations + string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_extra_whitespace(string):\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", string).strip()\n",
    "\n",
    "\n",
    "def cleaning (text):\n",
    "  # 1.removing extra spaces\n",
    "    text = re.sub(\"s+\",\" \", text)\n",
    "\n",
    "  # 2.remove repeating char\n",
    "    text= re.sub(r'(.)\\1+', r'\\1', text)  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e79390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:15.437138Z",
     "iopub.status.busy": "2022-01-24T21:16:15.436391Z",
     "iopub.status.idle": "2022-01-24T21:16:21.958424Z",
     "shell.execute_reply": "2022-01-24T21:16:21.958923Z",
     "shell.execute_reply.started": "2022-01-24T21:02:01.533651Z"
    },
    "papermill": {
     "duration": 6.553963,
     "end_time": "2022-01-24T21:16:21.959097",
     "exception": false,
     "start_time": "2022-01-24T21:16:15.405134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                            comment\n",
      "0   1         لقح نعم لنه فبر , ناس في مصر را كتخلص عليه\n",
      "1   2                          اثب لقح اهم في لحد من وفي\n",
      "2   3  انا لقح جوج رات حمد لله عند نعه ديل حيت درت لق...\n",
      "3   4  كنا تكد من ان جلل ملك جعل لقح مجا لكل غرب وحت ...\n",
      "4   5  شعب انا شبع ثقف خرف مءامره… ندم عندو هني يضر ل...\n",
      "   ID                                            comment  label\n",
      "0   1  انا اوص من هذا نبر لكل توج الي ركز لقح صدقو ام...      1\n",
      "1   2  هناك كثر لا فهم قصد كورو ليس صعب علي شبب لكن ص...      1\n",
      "2   3  حمد لله رقم قبل قرن بدل طقه لول ظهر كورو تحر ه...      1\n",
      "3   4  انا شخص اءد ما فرض سلط من ضرر دلء جوز لقح بهذا...      1\n",
      "4   5  نفس لشء في دين رشد ركز لقح غلق الي غيه اثن اين...      1\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_train.iterrows():\n",
    "    #row['comment'] = removeStopWords(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = remove_english_characters(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    #row['comment'] = remove_punctuations(row['comment'])\n",
    "    #row['comment'] = row['comment'].translate(translator)\n",
    "    #row['comment'] = cleaning(row['comment'])\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    #row['comment'] = removeLetters(row['comment'])\n",
    "    #row['comment'] = remove_extra_whitespace(row['comment'])\n",
    "    new_df_train = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df_train.update(new_df_train)\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    #row['comment'] = removeStopWords(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = remove_english_characters(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    #row['comment'] = remove_punctuations(row['comment'])\n",
    "    #row['comment'] = row['comment'].translate(translator)\n",
    "    #row['comment'] = cleaning(row['comment'])\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    #row['comment'] = removeNoise(row['comment'])\n",
    "    #row['comment'] = removeLetters(row['comment'])\n",
    "    #row['comment'] = remove_extra_whitespace(row['comment'])\n",
    "    new_df_test = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df_test.update(new_df_test)\n",
    "    \n",
    "print(df_test.head())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796677ac",
   "metadata": {
    "papermill": {
     "duration": 0.014257,
     "end_time": "2022-01-24T21:16:21.987963",
     "exception": false,
     "start_time": "2022-01-24T21:16:21.973706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Features extraction (Tfidf) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59aae94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:22.061387Z",
     "iopub.status.busy": "2022-01-24T21:16:22.056111Z",
     "iopub.status.idle": "2022-01-24T21:16:22.740189Z",
     "shell.execute_reply": "2022-01-24T21:16:22.739492Z",
     "shell.execute_reply.started": "2022-01-24T21:03:40.266772Z"
    },
    "papermill": {
     "duration": 0.737839,
     "end_time": "2022-01-24T21:16:22.740334",
     "exception": false,
     "start_time": "2022-01-24T21:16:22.002495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 52967)\n",
      "(240, 52967)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True , norm='l2', ngram_range=(1, 2))\n",
    "\n",
    "features_train = tfidf.fit_transform(df_train['comment']).toarray()\n",
    "labels_train = df_train['label']\n",
    "\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(df_test['comment']).toarray()\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199f70f",
   "metadata": {
    "papermill": {
     "duration": 0.014652,
     "end_time": "2022-01-24T21:16:22.770069",
     "exception": false,
     "start_time": "2022-01-24T21:16:22.755417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chi2 sur tf-idf features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992e819e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:22.805603Z",
     "iopub.status.busy": "2022-01-24T21:16:22.804931Z",
     "iopub.status.idle": "2022-01-24T21:16:23.999981Z",
     "shell.execute_reply": "2022-01-24T21:16:23.999417Z",
     "shell.execute_reply.started": "2022-01-24T21:06:14.572475Z"
    },
    "papermill": {
     "duration": 1.21517,
     "end_time": "2022-01-24T21:16:24.000142",
     "exception": false,
     "start_time": "2022-01-24T21:16:22.784972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "c=SelectKBest(chi2, k= 900)\n",
    "\n",
    "features_train= c.fit_transform(abs(features_train), labels_train)\n",
    "features_test= c.transform(abs(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4361fa9",
   "metadata": {
    "papermill": {
     "duration": 0.014558,
     "end_time": "2022-01-24T21:16:24.030158",
     "exception": false,
     "start_time": "2022-01-24T21:16:24.015600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Le classifieur SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2421df0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:24.065496Z",
     "iopub.status.busy": "2022-01-24T21:16:24.064792Z",
     "iopub.status.idle": "2022-01-24T21:16:27.328721Z",
     "shell.execute_reply": "2022-01-24T21:16:27.329309Z",
     "shell.execute_reply.started": "2022-01-24T21:08:50.907441Z"
    },
    "papermill": {
     "duration": 3.282223,
     "end_time": "2022-01-24T21:16:27.329498",
     "exception": false,
     "start_time": "2022-01-24T21:16:24.047275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='sigmoid')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(C=1, gamma='scale' , kernel = 'sigmoid')\n",
    "model.fit(features_train , labels_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de129c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:27.364467Z",
     "iopub.status.busy": "2022-01-24T21:16:27.363420Z",
     "iopub.status.idle": "2022-01-24T21:16:27.731013Z",
     "shell.execute_reply": "2022-01-24T21:16:27.730433Z",
     "shell.execute_reply.started": "2022-01-24T21:09:09.960544Z"
    },
    "papermill": {
     "duration": 0.38563,
     "end_time": "2022-01-24T21:16:27.731160",
     "exception": false,
     "start_time": "2022-01-24T21:16:27.345530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_svc = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67ba8ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:27.766574Z",
     "iopub.status.busy": "2022-01-24T21:16:27.765892Z",
     "iopub.status.idle": "2022-01-24T21:16:27.787941Z",
     "shell.execute_reply": "2022-01-24T21:16:27.787349Z",
     "shell.execute_reply.started": "2022-01-24T21:12:44.686832Z"
    },
    "papermill": {
     "duration": 0.041639,
     "end_time": "2022-01-24T21:16:27.788078",
     "exception": false,
     "start_time": "2022-01-24T21:16:27.746439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b42a056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:16:27.828674Z",
     "iopub.status.busy": "2022-01-24T21:16:27.827960Z",
     "iopub.status.idle": "2022-01-24T21:16:27.833152Z",
     "shell.execute_reply": "2022-01-24T21:16:27.832613Z",
     "shell.execute_reply.started": "2022-01-24T21:15:25.215819Z"
    },
    "papermill": {
     "duration": 0.029614,
     "end_time": "2022-01-24T21:16:27.833295",
     "exception": false,
     "start_time": "2022-01-24T21:16:27.803681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission['label']= y_pred_svc \n",
    "submission = submission[['ID','label']]\n",
    "submission.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e6407",
   "metadata": {
    "papermill": {
     "duration": 0.014993,
     "end_time": "2022-01-24T21:16:27.863693",
     "exception": false,
     "start_time": "2022-01-24T21:16:27.848700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.591751,
   "end_time": "2022-01-24T21:16:31.394668",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-24T21:15:57.802917",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
