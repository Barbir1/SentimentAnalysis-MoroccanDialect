{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a182569c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.01298,
     "end_time": "2022-01-24T21:53:08.332556",
     "exception": false,
     "start_time": "2022-01-24T21:53:08.319576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importation des bibliothèques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5231a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:08.361409Z",
     "iopub.status.busy": "2022-01-24T21:53:08.360260Z",
     "iopub.status.idle": "2022-01-24T21:53:15.538951Z",
     "shell.execute_reply": "2022-01-24T21:53:15.538209Z",
     "shell.execute_reply.started": "2022-01-24T21:49:16.286813Z"
    },
    "papermill": {
     "duration": 7.193279,
     "end_time": "2022-01-24T21:53:15.539133",
     "exception": false,
     "start_time": "2022-01-24T21:53:08.345854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import svm\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77afc98",
   "metadata": {
    "papermill": {
     "duration": 0.011073,
     "end_time": "2022-01-24T21:53:15.561828",
     "exception": false,
     "start_time": "2022-01-24T21:53:15.550755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importation des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04d25cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:15.587302Z",
     "iopub.status.busy": "2022-01-24T21:53:15.586694Z",
     "iopub.status.idle": "2022-01-24T21:53:15.646748Z",
     "shell.execute_reply": "2022-01-24T21:53:15.645979Z",
     "shell.execute_reply.started": "2022-01-24T21:42:52.970641Z"
    },
    "papermill": {
     "duration": 0.073805,
     "end_time": "2022-01-24T21:53:15.646920",
     "exception": false,
     "start_time": "2022-01-24T21:53:15.573115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/test_stage1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c9250",
   "metadata": {
    "papermill": {
     "duration": 0.011538,
     "end_time": "2022-01-24T21:53:15.670079",
     "exception": false,
     "start_time": "2022-01-24T21:53:15.658541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2185cf19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:15.699778Z",
     "iopub.status.busy": "2022-01-24T21:53:15.698963Z",
     "iopub.status.idle": "2022-01-24T21:53:16.379327Z",
     "shell.execute_reply": "2022-01-24T21:53:16.379854Z",
     "shell.execute_reply.started": "2022-01-24T21:43:34.703153Z"
    },
    "papermill": {
     "duration": 0.698553,
     "end_time": "2022-01-24T21:53:16.380021",
     "exception": false,
     "start_time": "2022-01-24T21:53:15.681468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('arabic')\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "translator = str.maketrans('', '', punctuations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b007c9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:16.424558Z",
     "iopub.status.busy": "2022-01-24T21:53:16.406914Z",
     "iopub.status.idle": "2022-01-24T21:53:16.426688Z",
     "shell.execute_reply": "2022-01-24T21:53:16.427160Z",
     "shell.execute_reply.started": "2022-01-24T21:44:18.854307Z"
    },
    "papermill": {
     "duration": 0.035308,
     "end_time": "2022-01-24T21:53:16.427335",
     "exception": false,
     "start_time": "2022-01-24T21:53:16.392027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence_0 = [w for w in word_tokens if not w in stop_words] \n",
    "    text = ' '.join([i for i in filtered_sentence_0])\n",
    "    return text\n",
    "\n",
    "def removestpWords(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence_1 = [w for w in word_tokens if not w in stpWords ]  \n",
    "    text = ' '.join([i for i in filtered_sentence_1])\n",
    "    return text\n",
    "\n",
    "def removeNoise(text) :\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence_2 = [w for w in word_tokens if not w in noise]   \n",
    "    text = ' '.join([i for i in filtered_sentence_2])\n",
    "    return text\n",
    "\n",
    "def NormalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return text\n",
    "\n",
    "def arabic_diacritics(text):\n",
    "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    st = ISRIStemmer()\n",
    "    stemmed_words = []\n",
    "    word_tokens = word_tokenize(text) \n",
    "    for w in word_tokens:\n",
    "        stemmed_words.append(st.stem(w))\n",
    "    stemmed_words = \" \".join(stemmed_words)\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_english_characters(text):\n",
    "        #out = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "        out= re.sub(r'[a-zA-Z]+', '', text)\n",
    "        #out = re.sub(r\"\\n\", '', out)\n",
    "        #out = re.sub(r\"\\s+\", ' ', out)\n",
    "        #out = re.sub(r'[^\\u0600-\\u06FF]', ' ', out)\n",
    "        return out.strip() \n",
    "\n",
    "\n",
    "def removeLetters(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if len(w)>1] \n",
    "    text = ' '.join([i for i in filtered_sentence])\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations + string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_extra_whitespace(string):\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", string).strip()\n",
    "\n",
    "\n",
    "def cleaning (text):\n",
    "  # 1.removing extra spaces\n",
    "    text = re.sub(\"s+\",\" \", text)\n",
    "\n",
    "  # 2.remove repeating char\n",
    "    text= re.sub(r'(.)\\1+', r'\\1', text)  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b32093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:16.454446Z",
     "iopub.status.busy": "2022-01-24T21:53:16.453795Z",
     "iopub.status.idle": "2022-01-24T21:53:24.172437Z",
     "shell.execute_reply": "2022-01-24T21:53:24.173196Z",
     "shell.execute_reply.started": "2022-01-24T21:46:26.070770Z"
    },
    "papermill": {
     "duration": 7.73402,
     "end_time": "2022-01-24T21:53:24.173443",
     "exception": false,
     "start_time": "2022-01-24T21:53:16.439423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                            comment\n",
      "0   1        لقح نعم لنه فابورالناس في مصر را كتخلص عليه\n",
      "1   2                          اثب لقح اهم في لحد من وفي\n",
      "2   3  انا لقح جوج رات حمد له عند نعه ديل حيت درت لقح...\n",
      "3   4  كنا تكد من ان جلل ملك جعل لقح مجا لكل غرب وحت ...\n",
      "4   5  شعب انا شبع ثقف خرف ءمر ندم عندو هني يضر ملا ا...\n",
      "   ID                                            comment  label\n",
      "0   1  انا اوص من هذا نبر لكل لتج الي ركز لقح صدقو ام...      1\n",
      "1   2  هناك كثر لا فهم قصد كورو ليس صعب علي شبب لكن ص...      1\n",
      "2   3  حمد له رقم قبل قرن بدل طقه لول ظهر كورو تحر هن...      1\n",
      "3   4  انا شخص اءد ما فرض سلط من ضرر دلء جوز لقح بهذا...      1\n",
      "4   5  نفس لشء في دين رشد ركز لقح غلق الي غيه اثن اين...      1\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_train.iterrows():\n",
    "    #row['comment'] = removeStopWords(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = remove_english_characters(row['comment'])\n",
    "    row['comment'] = row['comment'].translate(translator)\n",
    "    row['comment'] = remove_punctuations(row['comment'])\n",
    "    row['comment'] = removeLetters(row['comment'])\n",
    "    row['comment'] = cleaning(row['comment'])\n",
    "    row['comment'] = remove_extra_whitespace(row['comment'])\n",
    "    row['comment'] = stemming(row['comment'])\n",
    "    new_df_train = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df_train.update(new_df_train)\n",
    "    \n",
    "for index, row in df_test.iterrows():\n",
    "    #row['comment'] = removeStopWords(row['comment'])\n",
    "    row['comment'] = NormalizeArabic(row['comment'])\n",
    "    row['comment'] = arabic_diacritics(row['comment'])\n",
    "    row['comment'] = removeNumbers(row['comment'])\n",
    "    row['comment'] = remove_english_characters(row['comment'])\n",
    "    row['comment'] = row['comment'].translate(translator)\n",
    "    row['comment'] = remove_punctuations(row['comment'])\n",
    "    row['comment'] = removeLetters(row['comment'])\n",
    "    row['comment'] = cleaning(row['comment'])\n",
    "    row['comment'] = remove_extra_whitespace(row['comment'])\n",
    "    row['comment'] = stemming(row['comment']) \n",
    "    new_df_test = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n",
    "    df_test.update(new_df_test)\n",
    "print(df_test.head())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e64f4a",
   "metadata": {
    "papermill": {
     "duration": 0.012231,
     "end_time": "2022-01-24T21:53:24.198559",
     "exception": false,
     "start_time": "2022-01-24T21:53:24.186328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Features extraction (Tfidf) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ea6bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:24.226242Z",
     "iopub.status.busy": "2022-01-24T21:53:24.225610Z",
     "iopub.status.idle": "2022-01-24T21:53:25.157040Z",
     "shell.execute_reply": "2022-01-24T21:53:25.157553Z",
     "shell.execute_reply.started": "2022-01-24T21:47:27.969412Z"
    },
    "papermill": {
     "duration": 0.947196,
     "end_time": "2022-01-24T21:53:25.157732",
     "exception": false,
     "start_time": "2022-01-24T21:53:24.210536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 51771)\n",
      "(240, 51771)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True , norm='l2', ngram_range=(1, 2))\n",
    "\n",
    "features_train = tfidf.fit_transform(df_train['comment']).toarray()\n",
    "labels_train = df_train['label']\n",
    "\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(df_test['comment']).toarray()\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb19f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:25.187477Z",
     "iopub.status.busy": "2022-01-24T21:53:25.186837Z",
     "iopub.status.idle": "2022-01-24T21:53:26.406125Z",
     "shell.execute_reply": "2022-01-24T21:53:26.407526Z",
     "shell.execute_reply.started": "2022-01-24T21:49:42.335954Z"
    },
    "papermill": {
     "duration": 1.237545,
     "end_time": "2022-01-24T21:53:26.407795",
     "exception": false,
     "start_time": "2022-01-24T21:53:25.170250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = LogisticRegression(random_state=0)\n",
    "model_lr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68cea28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:26.472194Z",
     "iopub.status.busy": "2022-01-24T21:53:26.471145Z",
     "iopub.status.idle": "2022-01-24T21:53:26.509074Z",
     "shell.execute_reply": "2022-01-24T21:53:26.510612Z",
     "shell.execute_reply.started": "2022-01-24T21:49:46.787709Z"
    },
    "papermill": {
     "duration": 0.079727,
     "end_time": "2022-01-24T21:53:26.510900",
     "exception": false,
     "start_time": "2022-01-24T21:53:26.431173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_lr = model_lr.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3c5d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:26.565795Z",
     "iopub.status.busy": "2022-01-24T21:53:26.564121Z",
     "iopub.status.idle": "2022-01-24T21:53:26.574106Z",
     "shell.execute_reply": "2022-01-24T21:53:26.575234Z",
     "shell.execute_reply.started": "2022-01-24T21:51:32.859379Z"
    },
    "papermill": {
     "duration": 0.040807,
     "end_time": "2022-01-24T21:53:26.575545",
     "exception": false,
     "start_time": "2022-01-24T21:53:26.534738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/sentiment-analysis-on-moroccan-arabic-dialect/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb68744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T21:53:26.636263Z",
     "iopub.status.busy": "2022-01-24T21:53:26.635611Z",
     "iopub.status.idle": "2022-01-24T21:53:26.641614Z",
     "shell.execute_reply": "2022-01-24T21:53:26.641093Z",
     "shell.execute_reply.started": "2022-01-24T21:52:35.212873Z"
    },
    "papermill": {
     "duration": 0.038584,
     "end_time": "2022-01-24T21:53:26.641760",
     "exception": false,
     "start_time": "2022-01-24T21:53:26.603176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission['label']= y_pred_lr \n",
    "submission = submission[['ID','label']]\n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.564768,
   "end_time": "2022-01-24T21:53:29.284754",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-24T21:52:58.719986",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
